{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" gp.py\n",
    "\n",
    "https://github.com/thuijskens/bayesian-optimization/blob/master/python/gp.py\n",
    "\n",
    "Bayesian optimisation of loss functions.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "\n",
    "    Expected improvement acquisition function.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data, target = make_classification(n_samples=2500,\n",
    "                                   n_features=45,\n",
    "                                   n_informative=15,\n",
    "                                   n_redundant=5)\n",
    "\n",
    "def sample_loss(params):\n",
    "    return cross_val_score(SVC(C=10 ** params[0], gamma=10 ** params[1], random_state=12345),\n",
    "                           X=data, y=target, scoring='roc_auc', cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58333333, -2.42105263])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up grid\n",
    "\n",
    "lambdas = np.linspace(1, -4, 25)\n",
    "gammas = np.linspace(1, -4, 20)\n",
    "\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[C, gamma] for gamma in gammas for C in lambdas])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "param_grid[np.array(real_loss).argmax(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEaCAYAAAAFaHxEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f245056390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('text', usetex=False)\n",
    "\n",
    "C, G = np.meshgrid(lambdas, gammas)\n",
    "plt.figure()\n",
    "cp = plt.contourf(C, G, np.array(real_loss).reshape(C.shape))\n",
    "plt.colorbar(cp)\n",
    "plt.title('Filled contours plot of loss function $\\mathcal{L}$($\\gamma$, $C$)')\n",
    "plt.xlabel('$C$')\n",
    "plt.ylabel('$\\gamma')\n",
    "#plt.savefig('/Users/thomashuijskens/Personal/gp-optimisation/figures/real_loss_contour.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.37360131e-05]), 'warnflag': 2, 'funcalls': 50, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 4}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "c:\\users\\chapm\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -4.73536597e-05]), 'warnflag': 2, 'funcalls': 50, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 4}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    }
   ],
   "source": [
    "bounds = np.array([[-4, 1], [-4, 1]])\n",
    "\n",
    "xp, yp = bayesian_optimisation(n_iters=30, \n",
    "                               sample_loss=sample_loss, \n",
    "                               bounds=bounds,\n",
    "                               n_pre_samples=3,\n",
    "                               random_search=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
