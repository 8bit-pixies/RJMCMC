{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this topic is to determine the ideal projection when the dimensions of the input may vary (in columns). The dimensions of the columns are determined by feature generation/embeddings. \n",
    "\n",
    "Let:\n",
    "\n",
    "*  $W$ be the projection matrix\n",
    "*  $X$ to be the matrix representing input data which may vary in dimension from iteration to iteration\n",
    "*  $Y$ be the result of the projection, i.e. $WX = Y$\n",
    "\n",
    "WLOG fix the size of $Y$, (let's call this $d$). Then we have $W \\in \\mathbb{R}^{d \\times n}$.\n",
    "\n",
    "In this setting within the RJMCMC framework, the number of parameters we are estimating is:\n",
    "\n",
    "*  all entries in $W$ ($d \\times n$)  \n",
    "*  all relevant combination of feature generation functions and their respective parameters \n",
    "                                             \n",
    "\n",
    "### Examples of feature vector generation functions\n",
    "\n",
    "As an example of an algorithm which we should compare is the multivariate adaptive regression splines (MARS)\n",
    "\n",
    "**Hinge Function**\n",
    "\n",
    "The hinge function ($f_\\text{hinge}$) is similiar to ReLu), can be defined as a feature generation function, (_probably not the right way to write this out_)\n",
    "\n",
    "$$ f_{\\text{hinge}^+} (X_{j}, \\theta) = (X_j-\\theta \\mathbf{1})_{+}$$\n",
    "\n",
    "Where $X_j$ represents the $j$th column, without loss of generality we can likewise define $f_{\\text{hinge}^-} (X_{j}, \\theta) = (\\theta \\mathbf{1} - X_j)_{+}$\n",
    "\n",
    "**Interaction**\n",
    "\n",
    "Interaction term will be defined the dot product of two feature vectors (which can be the same feature vector), (_probably not the right way to write this out_)\n",
    "\n",
    "$$ f_\\text{interaction} (X_1, X_2) = X_1  \\cdot X_2 $$\n",
    "\n",
    "Replicating MARS in RJMCMC\n",
    "--------------------------\n",
    "\n",
    "[MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines) is a prime candidate for exploring this problem. \n",
    "\n",
    "In the RJMCMC space we have the following decisions:\n",
    "\n",
    "1. GROW - we can grow the state by Proposing a new feature vector generation transformation and respective parameter, e.g. hinge, interaction, or more complex function transforms which could be a composition of function generation...\n",
    "2. DESTROY - we can also delete a created feature\n",
    "\n",
    "This would be represent moving from one state to another.                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook shows a sample pipeline that is to be learnt by feature discovery. \n",
    "\n",
    "In this example we will use 3 custom components:\n",
    "\n",
    "*  Restricted Boltzmann Machine (which can be replaced with artibitary feature reduction method)\n",
    "*  Hinge search (which can be replaced with any kind of feature discovery method, e.g. tfidf)\n",
    "*  Interaction term (to create features like x^2 or x1*x2)\n",
    "\n",
    "with these pieces we will be able to replicate (theoretically) models like MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hinge import Hinge, error_on_split\n",
    "from interaction import Interaction\n",
    "from rbm import *\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# using svm as per scikit-feature repo\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import random_projection\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "problem_setup = {\n",
    "    'n_samples': 300, \n",
    "    'n_features': 50, \n",
    "    'n_informative': 10, \n",
    "    'n_redundant': 10, \n",
    "    'n_classes': 3, \n",
    "    'random_state': 0    \n",
    "}\n",
    "\n",
    "X, y = make_classification(**problem_setup)\n",
    "X = X.astype(float)\n",
    "n_samples, n_features = X.shape    # number of samples and number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6900000000000001\n"
     ]
    }
   ],
   "source": [
    "# vanilla pipeline\n",
    "pipeline = []\n",
    "pipeline.append(('linear svm', LinearSVC()))\n",
    "model = Pipeline(pipeline)\n",
    "\n",
    "# split data into 10 folds\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: {}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6999999999999998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pipeline with pca\n",
    "pipeline = []\n",
    "pipeline.append(('pca', PCA()))\n",
    "pipeline.append(('linear svm', LinearSVC()))\n",
    "model = Pipeline(pipeline)\n",
    "\n",
    "# split data into 10 folds\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: {}\".format(results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7433333333333335\n"
     ]
    }
   ],
   "source": [
    "# pipeline with factor analysis\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# pipeline with pca\n",
    "pipeline = []\n",
    "pipeline.append(('pca', FactorAnalysis()))\n",
    "pipeline.append(('linear svm', LinearSVC()))\n",
    "model = Pipeline(pipeline)\n",
    "\n",
    "# split data into 10 folds\n",
    "kfold = KFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: {}\".format(results.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
