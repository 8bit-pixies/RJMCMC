{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.special import expit  # logistic function\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "# Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``\n",
    "from sklearn.utils.extmath import log_logistic \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just straight from scipy...\n",
    "# this is a simplified solution without\n",
    "# some of the \"nice\" optimization\n",
    "\n",
    "class BernoulliRBM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\n",
    "                 n_iter=10):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def transform(self, X):     \n",
    "        \"\"\"P(h=1|v=X)\n",
    "        \"\"\"\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Compute P(h=1|v)\n",
    "        \"\"\"\n",
    "        p = np.dot(v, self.components_.T)\n",
    "        p += self.intercept_hidden_\n",
    "        return expit(p, out=p)\n",
    "    \n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the distribution P(h|v).\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        # see hinton chpt 3\n",
    "        return (np.random.random_sample(size=p.shape) < p)\n",
    "    \n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        return (np.random.random_sample(size=p.shape) < p)\n",
    "    \n",
    "   \n",
    "    def gibbs(self, v):\n",
    "        \"\"\"Perform one Gibbs sampling step.\n",
    "        \"\"\"\n",
    "        h_ = self._sample_hiddens(v)\n",
    "        v_ = self._sample_visibles(h_)\n",
    "\n",
    "        return v_\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'components_'):\n",
    "            self.components_ = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'intercept_hidden_'):\n",
    "            self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'intercept_visible_'):\n",
    "            self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        if not hasattr(self, 'h_samples_'):\n",
    "            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X, self.random_state_)\n",
    "        \n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Stochastic Maximum Likelihood (SML).\n",
    "        \"\"\"\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        v_neg = self._sample_visibles(self.h_samples_)\n",
    "        h_neg = self._mean_hiddens(v_neg)\n",
    "\n",
    "        lr = float(self.learning_rate) / v_pos.shape[0]\n",
    "        update = np.dot(v_pos.T, h_pos).T\n",
    "        update -= np.dot(h_neg.T, v_neg)\n",
    "        self.components_ += lr * update\n",
    "        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.intercept_visible_ += lr * (np.asarray(\n",
    "                                         v_pos.sum(axis=0)).squeeze() -\n",
    "                                         v_neg.sum(axis=0))\n",
    "\n",
    "        h_neg[np.random.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n",
    "        self.h_samples_ = np.floor(h_neg, h_neg)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        self.components_ = np.random.normal(0, 0.01, (self.n_components, X.shape[1]))\n",
    "        self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        begin = time.time()\n",
    "        for iteration in range(1, self.n_iter + 1):\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=100, n_iter=10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randint(low=0, high=1, size=(100, 100))\n",
    "rbm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622],\n",
       "       [ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622],\n",
       "       [ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622],\n",
       "       ..., \n",
       "       [ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622],\n",
       "       [ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622],\n",
       "       [ 0.56985902,  0.57098287,  0.57070786, ...,  0.56883462,\n",
       "         0.56916813,  0.56876622]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
