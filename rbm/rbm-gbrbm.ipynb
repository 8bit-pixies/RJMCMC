{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.special import expit  # logistic function\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "# https://gist.github.com/dwf/359323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is just straight from scipy...\n",
    "# this is a simplified solution without\n",
    "# some of the \"nice\" optimization\n",
    "\n",
    "class GaussianBernoulliRBM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=256, learning_rate=0.1, batch_size=10,\n",
    "                 n_iter=10, sigma=1, verbose=False):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def _mean_visibles_given_hiddens(self, h, v):\n",
    "        \"\"\"Conditional probability derivation P(v|h).\n",
    "           P(v|h) = N( Wh + bias_vis, sigma^2)\n",
    "           Page 38 (http://www.ini.rub.de/data/documents/tns/masterthesis_janmelchior.pdf)\n",
    "        \"\"\"\n",
    "        p = (np.dot(h, self.components_)) + self.intercept_visible_\n",
    "\n",
    "        return norm.rvs(loc=p, scale=np.square(self.sigma))\n",
    "    \n",
    "    def reconstruct(self, v):\n",
    "        \"\"\"reconstruct by computing positive phase\n",
    "           followed by the negative phase\n",
    "        \"\"\"\n",
    "        h_ = self._sample_hiddens(v)\n",
    "        v_ = self._mean_visibles_given_hiddens(h_, v)\n",
    "        return v_\n",
    "    \n",
    "    def _sigma_gradient(self, v, h):\n",
    "        \"\"\"\n",
    "            Computes the partial derivative with\n",
    "            respect to sigma\n",
    "            Page 41 (http://www.ini.rub.de/data/documents/tns/masterthesis_janmelchior.pdf)\n",
    "        \"\"\"\n",
    "        t1 = (np.square(v - self.intercept_visible_) /\n",
    "              np.power(self.sigma_, 3))\n",
    "\n",
    "        t2 = (2 * v) / np.power(self.sigma_, 3)\n",
    "\n",
    "        t3 = safe_sparse_dot(h, self.components_)\n",
    "\n",
    "        return check_array(t1 - (t2 * t3))\n",
    "    \n",
    "    def transform(self, X):     \n",
    "        \"\"\"P(h=1|v=X)\n",
    "        \"\"\"\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Compute P(h=1|v)\n",
    "        \"\"\"\n",
    "        \n",
    "        p = np.dot(v, self.components_.T)\n",
    "        p += self.intercept_hidden_\n",
    "        return expit(p, out=p)\n",
    "    \n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the distribution P(h|v).\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        # see hinton chpt 3\n",
    "        return (np.random.random_sample(size=p.shape) < p)\n",
    "    \n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        \"\"\"\n",
    "        p = np.dot(h, self.components_)\n",
    "        p += self.intercept_visible_\n",
    "        expit(p, out=p)\n",
    "        return (np.random.random_sample(size=p.shape) < p)\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the free energy F(v) = - log sum_h exp(-E(v,h)).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        return (- np.dot(v, self.intercept_visible_)\n",
    "                - np.logaddexp(0, np.dot(v, self.components_.T)\n",
    "                               + self.intercept_hidden_).sum(axis=1))\n",
    "    \n",
    "    def gibbs(self, v):\n",
    "        \"\"\"Perform one Gibbs sampling step.\n",
    "        \"\"\"\n",
    "        h_ = self._sample_hiddens(v)\n",
    "        v_ = self._sample_visibles(h_)\n",
    "\n",
    "        return v_\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'components_'):\n",
    "            self.components_ = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'intercept_hidden_'):\n",
    "            self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'intercept_visible_'):\n",
    "            self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        if not hasattr(self, 'h_samples_'):\n",
    "            self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X, self.random_state_)\n",
    "        \n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Stochastic Maximum Likelihood (SML).\n",
    "        \"\"\"\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        v_neg = self._sample_visibles(self.h_samples_)\n",
    "        h_neg = self._mean_hiddens(v_neg)\n",
    "\n",
    "        lr = float(self.learning_rate) / v_pos.shape[0]\n",
    "        update = np.dot(v_pos.T, h_pos).T\n",
    "        update -= np.dot(h_neg.T, v_neg)\n",
    "        self.components_ += lr * update\n",
    "        self.intercept_hidden_ += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "        self.intercept_visible_ += lr * (np.asarray(\n",
    "                                         v_pos.sum(axis=0)).squeeze() -\n",
    "                                         v_neg.sum(axis=0))\n",
    "\n",
    "        h_neg[np.random.uniform(size=h_neg.shape) < h_neg] = 1.0  # sample binomial\n",
    "        self.h_samples_ = np.floor(h_neg, h_neg)\n",
    "        \n",
    "    def score_samples(self, X):\n",
    "        \"\"\"compute pseudo-likelihood X\"\"\"\n",
    "        v = X\n",
    "        #v = check_array(X, accept_sparse='csr')\n",
    "        #rng = check_random_state(self.random_state)\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in v.\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               np.random.randint(0, v.shape[1], v.shape[0]))\n",
    "\n",
    "        v_ = v.copy()\n",
    "        v_[ind] = 1 - v_[ind]\n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        self.components_ = np.random.normal(0, 0.01, (self.n_components, X.shape[1]))\n",
    "        self.intercept_hidden_ = np.zeros(self.n_components, )\n",
    "        self.intercept_visible_ = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        verbose = self.verbose\n",
    "        begin = time.time()\n",
    "        for iteration in range(1, self.n_iter + 1):\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=100, n_iter=10,\n",
       "       verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randint(low=0, high=1, size=(100000, 100))\n",
    "rbm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964],\n",
       "       [ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964],\n",
       "       [ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964],\n",
       "       ..., \n",
       "       [ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964],\n",
       "       [ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964],\n",
       "       [ 0.68772362,  0.68763322,  0.68757298, ...,  0.6871598 ,\n",
       "         0.68761404,  0.68850964]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
